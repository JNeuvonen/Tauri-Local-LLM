# Tauri-Local-LLM

Probably developing this from time to time. Tauri and Rust are cool ways to develop stuff with local LLMs. Desktop applications have easy access to the file system, which, in my opinion, probably makes it worth it to develop UI's to local LLM applications on Tauri instead of the web. Also Tauri or Electron can autoboot the server that the LLM is dependant on. 

Some nice people have also ported [llama.cpp](https://github.com/ggerganov/llama.cpp) to [Rust](https://github.com/rustformers/llm).

